{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\2024t2\\archive\\archive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [01:18<00:00,  9.53it/s]\n",
      "100%|██████████| 951/951 [02:49<00:00,  5.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Set the working directory\n",
    "# os.chdir('./')\n",
    "\n",
    "# Paths to the directories containing the violence and non-violence videos\n",
    "PATH_violence = \"./Real_Life_Violence_Dataset/Violence\"\n",
    "PATH_nonviolence = \"./Real_Life_Violence_Dataset/NonViolence\"\n",
    "\n",
    "# Create directories to save the extracted frames if they don't exist\n",
    "os.makedirs('./data/Violence', exist_ok=True)\n",
    "os.makedirs('./data/NonViolence', exist_ok=True)\n",
    "\n",
    "# Function to extract frames\n",
    "def extract_frames(video_path, save_dir):\n",
    "    for path in tqdm(glob.glob(video_path + '/*')):\n",
    "        fname = os.path.basename(path).split('.')[0]  # Get the base name of the file without the extension\n",
    "        vidcap = cv2.VideoCapture(path)  # Initialize video capture object\n",
    "        success, image = vidcap.read()  # Read the first frame\n",
    "        count = 0  # Initialize frame counter\n",
    "\n",
    "        # Loop to read frames from the video\n",
    "        while success:\n",
    "            if count % 1000 == 0:  # Save every 100th frame\n",
    "                # Save the frame as a JPEG file\n",
    "                cv2.imwrite(f\"{save_dir}/{fname}-{str(count).zfill(4)}.jpg\", image)\n",
    "            success, image = vidcap.read()  # Read the next frame\n",
    "            count += 1  # Increment the frame counter\n",
    "\n",
    "# Extract frames from violent and non-violent videos in parallel\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    future_violence = executor.submit(extract_frames, PATH_violence, './data/Violence')\n",
    "    future_nonviolence = executor.submit(extract_frames, PATH_nonviolence, './data/NonViolence')\n",
    "\n",
    "# Ensure all threads are completed\n",
    "future_violence.result()\n",
    "future_nonviolence.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "[INFO] loading images...\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y has 0 samples: array([], dtype=float64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 109\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Binarize the labels\u001b[39;00m\n\u001b[0;32m    108\u001b[0m lb \u001b[38;5;241m=\u001b[39m LabelBinarizer()\n\u001b[1;32m--> 109\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mlb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m labels \u001b[38;5;241m=\u001b[39m to_categorical(labels)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Save the preprocessed data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\comp9517\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:329\u001b[0m, in \u001b[0;36mLabelBinarizer.fit_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[0;32m    310\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit label binarizer/transform multi-class labels to binary labels.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m    The output of transform is sometimes referred to as\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m        will be of CSR format.\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(y)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\comp9517\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\comp9517\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:303\u001b[0m, in \u001b[0;36mLabelBinarizer.fit\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultioutput target data is not supported with label binarization\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m     )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my has 0 samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y)\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_input_ \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39missparse(y)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m unique_labels(y)\n",
      "\u001b[1;31mValueError\u001b[0m: y has 0 samples: array([], dtype=float64)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# Import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import AveragePooling2D, Dropout, Flatten, Dense, Input, LSTM, TimeDistributed\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pickle\n",
    "import cv2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# Argument dictionary\n",
    "args = {\n",
    "    \"dataset\": \"archive/archive/data\",\n",
    "    \"model\": \"E:/2024t2/violence_model.h5\",\n",
    "    \"label-bin\": \"E:/2024t2/lb.pickle\",\n",
    "    \"epochs\": 10,\n",
    "    \"plot\": \"E:/2024t2/plot.png\"\n",
    "}\n",
    "\n",
    "# Initialize the set of labels\n",
    "LABELS = set([\"Violence\", \"NonViolence\"])\n",
    "\n",
    "# Define file paths for preprocessed data\n",
    "data_file = 'E:\\2024t2\\archive\\archive\\data.npy'\n",
    "labels_file = 'E:\\2024t2\\archive\\archive\\labels.npy'\n",
    "\n",
    "if os.path.exists(data_file) and os.path.exists(labels_file):\n",
    "    # Load the preprocessed data\n",
    "    print(\"[INFO] loading preprocessed data...\")\n",
    "    data = np.load(data_file, allow_pickle=True)\n",
    "    labels = np.load(labels_file)\n",
    "else:\n",
    "    # Function to process a single image\n",
    "    def process_image(imagePath):\n",
    "        # Extract the class label from the filename\n",
    "        label = imagePath.split(os.path.sep)[-2]\n",
    "        if label not in LABELS:\n",
    "            return None, None\n",
    "\n",
    "        # Load the image, swap color channels, and resize it to be a fixed 224x224 pixels\n",
    "        image = cv2.imread(imagePath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    # Grab the list of images in our dataset directory\n",
    "    print('-' * 100)\n",
    "    print(\"[INFO] loading images...\")\n",
    "    print('-' * 100)\n",
    "    imagePaths = list(paths.list_images(args[\"dataset\"]))\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # Process images in batches to reduce memory usage\n",
    "    batch_size = 1000\n",
    "    for i in range(0, len(imagePaths), batch_size):\n",
    "        batchPaths = imagePaths[i:i + batch_size]\n",
    "\n",
    "        # Use ThreadPoolExecutor to process images in parallel\n",
    "        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "            results = list(tqdm(executor.map(process_image, batchPaths), total=len(batchPaths)))\n",
    "\n",
    "        # Filter out None values (if any)\n",
    "        data.extend([r[0] for r in results if r[0] is not None])\n",
    "        labels.extend([r[1] for r in results if r[1] is not None])\n",
    "\n",
    "    # Group frames by video\n",
    "    video_data = {}\n",
    "    for image, label in zip(data, labels):\n",
    "        if label not in video_data:\n",
    "            video_data[label] = []\n",
    "        video_data[label].append(image)\n",
    "\n",
    "    # Prepare the data and labels for LSTM\n",
    "    data = []\n",
    "    labels = []\n",
    "    for label in video_data:\n",
    "        # Pad sequences to have the same length\n",
    "        sequence = video_data[label]\n",
    "        sequence_length = max(len(seq) for seq in video_data.values())  # Find the longest sequence\n",
    "        sequence = np.pad(sequence, [(0, sequence_length - len(sequence)), (0, 0), (0, 0), (0, 0)], mode='constant')  # Pad shorter sequences\n",
    "        data.append(sequence)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Convert the data and labels to NumPy arrays\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Binarize the labels\n",
    "    lb = LabelBinarizer()\n",
    "    labels = lb.fit_transform(labels)\n",
    "    labels = to_categorical(labels)\n",
    "\n",
    "    # Save the preprocessed data\n",
    "    print(\"[INFO] saving preprocessed data...\")\n",
    "    np.save(data_file, data)\n",
    "    np.save(labels_file, labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "[INFO] loading images...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[DEBUG] Total image paths found: 0\n",
      "[DEBUG] Total images processed: 0\n",
      "[DEBUG] Total labels processed: 0\n",
      "[ERROR] No valid data or labels found.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No valid data or labels found. Please check your dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 94\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(labels) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ERROR] No valid data or labels found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid data or labels found. Please check your dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Group frames by video\u001b[39;00m\n\u001b[0;32m     97\u001b[0m video_data \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mValueError\u001b[0m: No valid data or labels found. Please check your dataset."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# Import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import AveragePooling2D, Dropout, Flatten, Dense, Input, LSTM, TimeDistributed\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pickle\n",
    "import cv2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# Argument dictionary\n",
    "args = {\n",
    "    \"dataset\": \"archive/archive/real life violence situations/Real Life Violence Dataset\",\n",
    "    \"model\": \"E:/2024t2/violence_model.h5\",\n",
    "    \"label-bin\": \"E:/2024t2/lb.pickle\",\n",
    "    \"epochs\": 10,\n",
    "    \"plot\": \"E:/2024t2/plot.png\"\n",
    "}\n",
    "\n",
    "# Initialize the set of labels\n",
    "LABELS = set([\"Violence\", \"NonViolence\"])\n",
    "\n",
    "# Define file paths for preprocessed data\n",
    "data_file = 'E:/2024t2/data.npy'\n",
    "labels_file = 'E:/2024t2/labels.npy'\n",
    "\n",
    "if os.path.exists(data_file) and os.path.exists(labels_file):\n",
    "    # Load the preprocessed data\n",
    "    print(\"[INFO] loading preprocessed data...\")\n",
    "    data = np.load(data_file, allow_pickle=True)\n",
    "    labels = np.load(labels_file)\n",
    "else:\n",
    "    # Function to process a single image\n",
    "    def process_image(imagePath):\n",
    "        # Extract the class label from the filename\n",
    "        label = imagePath.split(os.path.sep)[-2]\n",
    "        if label not in LABELS:\n",
    "            return None, None\n",
    "\n",
    "        # Load the image, swap color channels, and resize it to be a fixed 224x224 pixels\n",
    "        image = cv2.imread(imagePath)\n",
    "        if image is None:\n",
    "            return None, None\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    # Grab the list of images in our dataset directory\n",
    "    print('-' * 100)\n",
    "    print(\"[INFO] loading images...\")\n",
    "    print('-' * 100)\n",
    "    imagePaths = list(paths.list_images(args[\"dataset\"]))\n",
    "    print(f\"[DEBUG] Total image paths found: {len(imagePaths)}\")\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # Process images in batches to reduce memory usage\n",
    "    batch_size = 1000\n",
    "    for i in range(0, len(imagePaths), batch_size):\n",
    "        batchPaths = imagePaths[i:i + batch_size]\n",
    "\n",
    "        # Use ThreadPoolExecutor to process images in parallel\n",
    "        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "            results = list(tqdm(executor.map(process_image, batchPaths), total=len(batchPaths)))\n",
    "\n",
    "        # Filter out None values (if any)\n",
    "        data.extend([r[0] for r in results if r[0] is not None])\n",
    "        labels.extend([r[1] for r in results if r[1] is not None])\n",
    "\n",
    "    print(f\"[DEBUG] Total images processed: {len(data)}\")\n",
    "    print(f\"[DEBUG] Total labels processed: {len(labels)}\")\n",
    "\n",
    "    if len(data) == 0 or len(labels) == 0:\n",
    "        print(\"[ERROR] No valid data or labels found.\")\n",
    "        raise ValueError(\"No valid data or labels found. Please check your dataset.\")\n",
    "\n",
    "    # Group frames by video\n",
    "    video_data = {}\n",
    "    for image, label in zip(data, labels):\n",
    "        if label not in video_data:\n",
    "            video_data[label] = []\n",
    "        video_data[label].append(image)\n",
    "\n",
    "    print(f\"[DEBUG] Video data: {len(video_data)} classes\")\n",
    "\n",
    "    # Prepare the data and labels for LSTM\n",
    "    data = []\n",
    "    labels = []\n",
    "    for label in video_data:\n",
    "        # Pad sequences to have the same length\n",
    "        sequence = video_data[label]\n",
    "        sequence_length = max(len(seq) for seq in video_data.values())  # Find the longest sequence\n",
    "        sequence = np.pad(sequence, [(0, sequence_length - len(sequence)), (0, 0), (0, 0), (0, 0)], mode='constant')  # Pad shorter sequences\n",
    "        data.append(sequence)\n",
    "        labels.append(label)\n",
    "\n",
    "    print(f\"[DEBUG] Data after padding: {len(data)} sequences\")\n",
    "    print(f\"[DEBUG] Labels after padding: {len(labels)} sequences\")\n",
    "\n",
    "    # Convert the data and labels to NumPy arrays\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Binarize the labels\n",
    "    lb = LabelBinarizer()\n",
    "    labels = lb.fit_transform(labels)\n",
    "    labels = to_categorical(labels)\n",
    "\n",
    "    # Save the preprocessed data\n",
    "    print(\"[INFO] saving preprocessed data...\")\n",
    "    np.save(data_file, data)\n",
    "    np.save(labels_file, labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "print(f\"[DEBUG] trainX shape: {trainX.shape}\")\n",
    "print(f\"[DEBUG] testX shape: {testX.shape}\")\n",
    "print(f\"[DEBUG] trainY shape: {trainY.shape}\")\n",
    "print(f\"[DEBUG] testY shape: {testY.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 10, 64, 64, 3)]   0         \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDi  (None, 10, 62, 62, 32)    896       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDi  (None, 10, 31, 31, 32)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDi  (None, 10, 30752)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                7889152   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7894273 (30.11 MB)\n",
      "Trainable params: 7894273 (30.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 5s 311ms/step - loss: 0.7182 - accuracy: 0.5474 - val_loss: 0.7010 - val_accuracy: 0.4412\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 2s 237ms/step - loss: 0.6926 - accuracy: 0.5474 - val_loss: 0.6891 - val_accuracy: 0.4118\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 2s 246ms/step - loss: 0.6938 - accuracy: 0.5620 - val_loss: 0.6708 - val_accuracy: 0.6471\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 2s 237ms/step - loss: 0.6782 - accuracy: 0.5766 - val_loss: 0.7073 - val_accuracy: 0.4118\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 2s 241ms/step - loss: 0.6651 - accuracy: 0.5474 - val_loss: 0.6786 - val_accuracy: 0.5882\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 3s 363ms/step - loss: 0.6235 - accuracy: 0.6496 - val_loss: 0.6835 - val_accuracy: 0.6176\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 2s 235ms/step - loss: 0.5299 - accuracy: 0.8029 - val_loss: 0.7227 - val_accuracy: 0.4412\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 2s 246ms/step - loss: 0.4607 - accuracy: 0.8686 - val_loss: 0.7625 - val_accuracy: 0.3824\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 2s 234ms/step - loss: 0.3936 - accuracy: 0.8978 - val_loss: 0.8734 - val_accuracy: 0.3529\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 2s 239ms/step - loss: 0.3435 - accuracy: 0.8978 - val_loss: 0.8006 - val_accuracy: 0.4118\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.8006 - accuracy: 0.4118\n",
      "Test Loss: 0.8005945682525635, Test Accuracy: 0.4117647111415863\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "\n",
    "# 定义数据集路径\n",
    "data_dir = 'E:\\\\2024t2\\\\archive\\\\archive\\\\data'\n",
    "\n",
    "# 定义暴力和非暴力文件夹路径\n",
    "nonviolence_dir = os.path.join(data_dir, 'NonViolence')\n",
    "violence_dir = os.path.join(data_dir, 'Violence')\n",
    "\n",
    "# 定义图像大小和序列长度\n",
    "img_height, img_width = 64, 64\n",
    "sequence_length = 10\n",
    "\n",
    "# 加载图像数据\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in glob.glob(os.path.join(folder, '*.jpg')):\n",
    "        img = Image.open(filename)\n",
    "        img = img.resize((img_width, img_height))\n",
    "        img = np.array(img)\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# 加载数据集\n",
    "nonviolence_images = load_images_from_folder(nonviolence_dir)\n",
    "violence_images = load_images_from_folder(violence_dir)\n",
    "\n",
    "# 创建标签\n",
    "nonviolence_labels = np.zeros(len(nonviolence_images))\n",
    "violence_labels = np.ones(len(violence_images))\n",
    "\n",
    "# 合并数据和标签\n",
    "X = np.concatenate((nonviolence_images, violence_images), axis=0)\n",
    "y = np.concatenate((nonviolence_labels, violence_labels), axis=0)\n",
    "\n",
    "# 打乱数据集\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# 规范化图像数据\n",
    "X = X / 255.0\n",
    "\n",
    "# 确保数据的数量是 sequence_length 的整数倍\n",
    "num_samples = (len(X) // sequence_length) * sequence_length\n",
    "X = X[:num_samples]\n",
    "y = y[:num_samples]\n",
    "\n",
    "# 将数据划分为训练集和测试集\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# 确保训练集和测试集的大小也适应 sequence_length 的整数倍\n",
    "train_samples = (len(X_train) // sequence_length) * sequence_length\n",
    "test_samples = (len(X_test) // sequence_length) * sequence_length\n",
    "\n",
    "X_train = X_train[:train_samples].reshape(-1, sequence_length, img_height, img_width, 3)\n",
    "X_test = X_test[:test_samples].reshape(-1, sequence_length, img_height, img_width, 3)\n",
    "\n",
    "# 由于我们是处理时间序列问题，我们需要在每个时间序列上分配一个标签\n",
    "# 我们使用序列中的第一个标签作为序列的标签\n",
    "y_train = y_train[:train_samples:sequence_length]\n",
    "y_test = y_test[:test_samples:sequence_length]\n",
    "\n",
    "# 构建模型\n",
    "input_shape = (sequence_length, img_height, img_width, 3)\n",
    "\n",
    "# 输入层\n",
    "input_layer = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "# TimeDistributed的卷积层\n",
    "cnn_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))(input_layer)\n",
    "cnn_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D((2, 2)))(cnn_layer)\n",
    "cnn_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())(cnn_layer)\n",
    "\n",
    "# LSTM层\n",
    "lstm_layer = tf.keras.layers.LSTM(64)(cnn_layer)\n",
    "\n",
    "# 全连接层\n",
    "dense_layer = tf.keras.layers.Dense(64, activation='relu')(lstm_layer)\n",
    "dropout_layer = tf.keras.layers.Dropout(0.5)(dense_layer)\n",
    "\n",
    "# 输出层\n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(dropout_layer)\n",
    "\n",
    "# 定义模型\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 打印模型摘要\n",
    "model.summary()\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# 评估模型\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 10, 64, 64, 3)]   0         \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDi  (None, 10, 62, 62, 32)    896       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_10 (TimeD  (None, 10, 31, 31, 32)    0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_11 (TimeD  (None, 10, 30752)         0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                7889152   \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7894273 (30.11 MB)\n",
      "Trainable params: 7894273 (30.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 5s 298ms/step - loss: 0.7333 - accuracy: 0.5036 - val_loss: 0.6613 - val_accuracy: 0.6765\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 2s 242ms/step - loss: 0.6958 - accuracy: 0.5620 - val_loss: 0.6651 - val_accuracy: 0.6765\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 3s 372ms/step - loss: 0.6495 - accuracy: 0.6058 - val_loss: 0.6595 - val_accuracy: 0.6765\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 2s 243ms/step - loss: 0.6332 - accuracy: 0.6642 - val_loss: 0.6570 - val_accuracy: 0.6765\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 2s 239ms/step - loss: 0.6450 - accuracy: 0.6131 - val_loss: 0.6686 - val_accuracy: 0.6176\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 2s 247ms/step - loss: 0.5971 - accuracy: 0.6496 - val_loss: 0.6331 - val_accuracy: 0.6765\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 2s 237ms/step - loss: 0.5746 - accuracy: 0.7007 - val_loss: 0.6528 - val_accuracy: 0.6471\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 2s 240ms/step - loss: 0.5330 - accuracy: 0.7080 - val_loss: 0.7600 - val_accuracy: 0.4412\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 2s 251ms/step - loss: 0.4977 - accuracy: 0.7810 - val_loss: 0.6586 - val_accuracy: 0.6471\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6331 - accuracy: 0.6765\n",
      "Test Loss: 0.6330539584159851, Test Accuracy: 0.6764705777168274\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "\n",
    "# 定义数据集路径\n",
    "data_dir = 'E:\\\\2024t2\\\\archive\\\\archive\\\\data'\n",
    "\n",
    "# 定义暴力和非暴力文件夹路径\n",
    "nonviolence_dir = os.path.join(data_dir, 'NonViolence')\n",
    "violence_dir = os.path.join(data_dir, 'Violence')\n",
    "\n",
    "# 定义图像大小和序列长度\n",
    "img_height, img_width = 64, 64\n",
    "sequence_length = 10\n",
    "\n",
    "# 加载图像数据\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in glob.glob(os.path.join(folder, '*.jpg')):\n",
    "        img = Image.open(filename)\n",
    "        img = img.resize((img_width, img_height))\n",
    "        img = np.array(img)\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# 加载数据集\n",
    "nonviolence_images = load_images_from_folder(nonviolence_dir)\n",
    "violence_images = load_images_from_folder(violence_dir)\n",
    "\n",
    "# 创建标签\n",
    "nonviolence_labels = np.zeros(len(nonviolence_images))\n",
    "violence_labels = np.ones(len(violence_images))\n",
    "\n",
    "# 合并数据和标签\n",
    "X = np.concatenate((nonviolence_images, violence_images), axis=0)\n",
    "y = np.concatenate((nonviolence_labels, violence_labels), axis=0)\n",
    "\n",
    "# 打乱数据集\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# 规范化图像数据\n",
    "X = X / 255.0\n",
    "\n",
    "# 确保数据的数量是 sequence_length 的整数倍\n",
    "num_samples = (len(X) // sequence_length) * sequence_length\n",
    "X = X[:num_samples]\n",
    "y = y[:num_samples]\n",
    "\n",
    "# 将数据划分为训练集和测试集\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# 确保训练集和测试集的大小也适应 sequence_length 的整数倍\n",
    "train_samples = (len(X_train) // sequence_length) * sequence_length\n",
    "test_samples = (len(X_test) // sequence_length) * sequence_length\n",
    "\n",
    "X_train = X_train[:train_samples].reshape(-1, sequence_length, img_height, img_width, 3)\n",
    "X_test = X_test[:test_samples].reshape(-1, sequence_length, img_height, img_width, 3)\n",
    "\n",
    "# 由于我们是处理时间序列问题，我们需要在每个时间序列上分配一个标签\n",
    "# 我们使用序列中的第一个标签作为序列的标签\n",
    "y_train = y_train[:train_samples:sequence_length]\n",
    "y_test = y_test[:test_samples:sequence_length]\n",
    "\n",
    "# 构建模型\n",
    "input_shape = (sequence_length, img_height, img_width, 3)\n",
    "\n",
    "# 输入层\n",
    "input_layer = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "# TimeDistributed的卷积层\n",
    "cnn_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))(input_layer)\n",
    "cnn_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D((2, 2)))(cnn_layer)\n",
    "cnn_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())(cnn_layer)\n",
    "\n",
    "# LSTM层\n",
    "lstm_layer = tf.keras.layers.LSTM(64)(cnn_layer)\n",
    "\n",
    "# 全连接层\n",
    "dense_layer = tf.keras.layers.Dense(64, activation='relu')(lstm_layer)\n",
    "dropout_layer = tf.keras.layers.Dropout(0.5)(dense_layer)\n",
    "\n",
    "# 输出层\n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(dropout_layer)\n",
    "\n",
    "# 定义模型\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 打印模型摘要\n",
    "model.summary()\n",
    "\n",
    "# 定义早停回调\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=3, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=50,  # 设置一个较大的值，实际训练的epoch数由早停决定\n",
    "    batch_size=16, \n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# 评估模型\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp9517",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
